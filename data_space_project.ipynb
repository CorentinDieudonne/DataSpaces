{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_space_project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorentinDieudonne/DataSpaces/blob/master/data_space_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqjIvXq7yz95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install squarify\n",
        "!pip3 install sklearn\n",
        "!pip3 install pandas\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from scipy import stats\n",
        "import squarify as sq\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import scatter_matrix\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB,BernoulliNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn import decomposition\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                              AdaBoostClassifier)\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def make_meshgrid(x, y, h):\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    return xx, yy\n",
        "\n",
        "def group(y):\n",
        "    for i in range(len(y)):\n",
        "        if (('electro' in y[i]) | \n",
        "        ('step' in y[i]) |\n",
        "        ('complextro' in y[i]) |\n",
        "        ('room' in y[i]) | \n",
        "        ('dance' in y[i]) | \n",
        "        ('wave' in y[i]) | \n",
        "        ('edm' in y[i]) | \n",
        "        ('house' in y[i])):\n",
        "            y[i] = 'electro'\n",
        "        elif (('hip' in y[i]) | \n",
        "        ('latin' in y[i]) | \n",
        "        ('r&b' in y[i]) | \n",
        "        ('rap' in y[i]) | \n",
        "        ('soul' in y[i])):\n",
        "            y[i] = 'rap'\n",
        "        elif (('pop' in y[i]) | ('mellow' in y[i]) | ('band' in y[i])):\n",
        "            y[i] = 'pop'\n",
        "        else :\n",
        "            print(y[i])\n",
        "            y[i] = 'undefined'\n",
        "    return y\n",
        "\n",
        "filename='https://raw.githubusercontent.com/CorentinDieudonne/DataSpaces/master/top10s.csv'\n",
        "df=pd.read_csv(filename,encoding='ISO-8859-1')\n",
        "df = df.reset_index()\n",
        "\n",
        "df.rename(columns={'Track.Name':'track_name',\n",
        "'top genre' : 'genre',\n",
        "'live':'liveness',\n",
        "'acous' : 'acousticness',\n",
        "'nrgy' : 'energy',\n",
        "'dnce' : 'danceability',\n",
        "'val' : 'valence',\n",
        "'dur':'duration', \n",
        "'pop':'popularity',\n",
        "'spch':'speechiness'},inplace=True)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------\n",
        "genre_size=df.groupby('genre').size()\n",
        "genre_list=df['genre'].values.tolist()\n",
        "#Here we get the repartition of each genre in the dataset : we consider working with 27 different genres.\n",
        "#-------------------------------------------------------------------------------------------\n",
        "\n",
        "#choose features that are going to be exploited & data split into test and train : \n",
        "#-------------------------------------------------------------------------------------------\n",
        "allfeatures = ['popularity', 'acousticness', 'danceability', 'energy', 'liveness', 'speechiness', 'valence', 'dB', 'bpm']\n",
        "features = ['popularity', 'bpm']\n",
        "\n",
        "#genres = ['big room', 'british soul', 'electropop',]\n",
        "genres = genre_list\n",
        "\n",
        "#X = df.loc[df['genre'].isin(genres), features].values\n",
        "#y = df.loc[df['genre'].isin(genres), 'genre'].values\n",
        "\n",
        "X = df.loc[:, allfeatures].values\n",
        "y = df.loc[:, 'genre'].values\n",
        "y = group(y)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y)\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "\n",
        "targets = list(le.classes_)\n",
        "#-------------------------------------------------------------------------------------------\n",
        "\n",
        "def pca5():\n",
        "    #1st operation : PCA in order to know which components are the most representative of the dataset\n",
        "    #-------------------------------------------------------------------------------------------\n",
        "    # Fit on training set\n",
        "    pca = PCA(n_components=5)\n",
        "    pca.fit_transform(X_train)\n",
        "    principalComponents = pca.fit_transform(X_train)\n",
        "    print(pca.explained_variance_ratio_)\n",
        "    principalDf = pd.DataFrame(data = principalComponents\n",
        "                , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4','principal component 5'])\n",
        "\n",
        "    finalDf = pd.concat([principalDf, df[['genre']]], axis = 1)\n",
        "\n",
        "    fig = plt.figure(figsize = (8,8))\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "    ax.set_title('2 component PCA', fontsize = 20)\n",
        "\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(targets)))\n",
        "    for target, color in zip(targets,colors):\n",
        "        indicesToKeep = finalDf['genre'] == target\n",
        "        ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "                , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "                , c = color\n",
        "                , s = 10)\n",
        "    ax.legend(targets)\n",
        "    ax.grid()\n",
        "\n",
        "def rbf_k():\n",
        "    print(\"---------RBF K--------------\")\n",
        "    X = df.loc[:, features].values\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "    X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "    #2nd step : Apply SVM to the dataset by using a kernel trick, we perform first a grid search\n",
        "    C_range = np.linspace(1, 3, 4)\n",
        "    gamma_range = np.linspace(0.1, 3, 4)\n",
        "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
        "    print(param_grid)\n",
        "    grid = GridSearchCV(svm.SVC(kernel='rbf'), param_grid)\n",
        "    grid.fit(X, y)\n",
        "\n",
        "    # display optimal parameters.\n",
        "    print(\"The optimal parameters are : \" + str(grid.best_params_) + \" with a score of : \" + str(grid.best_score_))\n",
        "\n",
        "    classifiers = []\n",
        "    for C in C_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(C=C, gamma=gamma)\n",
        "            clf.fit(X_train, y_train)\n",
        "            classifiers.append((C, gamma, clf))\n",
        "    xx, yy = make_meshgrid(X[:, 0], X[:, 1], 0.1)\n",
        "\n",
        "    \"\"\"\n",
        "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.subplot(len(C_range), len(gamma_range))\n",
        "\n",
        "    # visualize parameter's effect on decision function\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    \"\"\"\n",
        "\n",
        "    for (k, (C, gamma, clf)) in enumerate(classifiers):\n",
        "        # evaluate decision function in a grid\n",
        "        #Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        # visualize decision function for these parameters\n",
        "        plt.figure()\n",
        "        plt.xlim(xx.min(), xx.max())\n",
        "        plt.ylim(yy.min(), yy.max())\n",
        "        plt.contourf(xx, yy, Z, c=plt.cm.Spectral)\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y, s=10,\n",
        "                    edgecolors='k')\n",
        "\n",
        "        # visualize parameter's effect on decision function\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "        plt.title(\"RBF Kernel with C  = \" + str(C) + \"and gamma = \" + str(gamma))\n",
        "    \n",
        "    #plt.savefig('/Users/victorseguin/Desktop/python_plots/plotrbfkerneloptimal.eps', format='eps')\n",
        "    #-------------------------------------------------------------------------------------------\n",
        "\n",
        "def decision_tree():\n",
        "    print(\"---------DECISION TREE--------------\")\n",
        "    X = df.loc[:,features].values\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "    X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "    classifier = DecisionTreeClassifier(max_depth=None)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    cost_complexity = classifier.cost_complexity_pruning_path(X_train, y_train)\n",
        "    ccp_alphas, impurities = cost_complexity.ccp_alphas, cost_complexity.impurities\n",
        "    fig, ax = plt.subplots()\n",
        "    print('The depth max of the classifier is: ' + str(classifier.tree_.max_depth))\n",
        "    print('The importance of the different parameter is equals to : '+ str(classifier.feature_importances_))\n",
        "    scores = classifier.score(X_test, y_test)\n",
        "    print('decision tree has a score of : '+ str(scores))\n",
        "\n",
        "    xx, yy = make_meshgrid(X[:, 0], X[:, 1], 0.1)\n",
        "\n",
        "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.title(\"Decision tree with features \"+ \n",
        "               features[0] + \" and \"+ features[1])\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y,\n",
        "                cmap=ListedColormap(['r', 'y', 'b']),\n",
        "                edgecolor='k', s=20)\n",
        "\n",
        "\n",
        "def random_forest():\n",
        "    print(\"---------RANDOM FOREST--------------\")\n",
        "    X = df.loc[:, features].values\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "    X_train = StandardScaler().fit_transform(X_train)\n",
        "    #24 = sqrt(600), best parameter is m = sqrt(p) for random forest classifier\n",
        "    classifier = RandomForestClassifier(n_estimators=24,warm_start=True,oob_score=True)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    scores = classifier.score(X_test, y_test)\n",
        "    print('The importance of the different parameter is equals to : '+ str(classifier.feature_importances_))\n",
        "    print('random forest has a score of : '+ str(scores))\n",
        "\n",
        "    xx, yy = make_meshgrid(X[:, 0], X[:, 1], 0.1)\n",
        "    estimator_alpha = 1.0 / len(classifier.estimators_)\n",
        "    plt.figure()\n",
        "    plt.title(\"Random tree with features \"+ \n",
        "              features[0] + \" and \"+ features[1])\n",
        "\n",
        "    for tree in classifier.estimators_:\n",
        "        Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=plt.cm.Spectral)\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y,\n",
        "                cmap=ListedColormap(['r', 'y', 'b']),\n",
        "                edgecolor='k', s=20)\n",
        "\n",
        "\n",
        "pca5()\n",
        "rbf_k()\n",
        "decision_tree()\n",
        "random_forest()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaOUrAHSA3oW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rbf_kernel():\n",
        "    print(\"===========START KERNEL ==========\")\n",
        "    X = df.loc[:, features].values\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    gamma = 1\n",
        "    scores = np.zeros(7, dtype=float)\n",
        "    i = 0\n",
        "\n",
        "    for c in C:\n",
        "        model = svm.SVC(C=c)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        xx, yy = make_meshgrid(X_train[:, 0], X_train[:, 1], 0.1)\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        print(\"C = \" + str(c))\n",
        "\n",
        "        plt.figure()\n",
        "        plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Spectral, edgecolors='k')\n",
        "        plt.xlim(xx.min(), xx.max())\n",
        "        plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "        #plt.xlabel(x_label)\n",
        "        #plt.ylabel(y_label)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "        plt.title(\"RBF Kernel with C = \" + str(c) + \" and gamma = \" + str(gamma))\n",
        "        #plt.savefig('/Users/victorseguin/Desktop/python_plots/plotrbfkernel' + '_C' + str(c) + '.eps', format='eps')\n",
        "        scores[i] = model.score(X_test, y_test)\n",
        "        print (model.score(X_test, y_test))\n",
        "        i += 1\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Scores of the RBF Kernel classifier according to the C number\")\n",
        "    plt.xlabel(\"C number\")\n",
        "    plt.ylabel(\"prediction score\")\n",
        "    plt.scatter(C, scores)\n",
        "    #plt.savefig('/Users/victorseguin/Desktop/plots/data_space_plot/scoresrbfkernel.eps', format='eps')\n",
        "    \n",
        "    indice = [i for i, x in enumerate(scores) if x == max(scores)]\n",
        "    print(\"best indice : \" + str(C[indice[0]]))\n",
        "\n",
        "    model = svm.SVC(kernel='rbf', gamma=2, C=C[indice[0]])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    score_test = model.score(X_test, y_test)\n",
        "    print(\"score on test set : \" + str(score_test))\n",
        "    \n",
        "    gamma_range = np.logspace(-2, 2, 10)\n",
        "    param_grid = {'C': C, 'gamma': gamma_range}\n",
        "\n",
        "    grid = model_selection.GridSearchCV(svm.SVC(kernel='rbf'), param_grid, scoring='accuracy')\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    # display optimal parameters.\n",
        "    print(\"The optimal parameters are : \" + str(grid.best_params_) + \" with a score of : \" + str(grid.best_score_))\n",
        "\n",
        "    classifier = svm.SVC(kernel='rbf', gamma=grid.best_params_['gamma'], C=grid.best_params_['C'])\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    # repeat grid search with the new merged set :\n",
        "    gamma_range = np.logspace(-2, 2, 10)\n",
        "    param_grid = {'C': C, 'gamma': gamma_range}\n",
        "\n",
        "    grid = model_selection.GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
        "    grid.fit(X_test, y_test)\n",
        "\n",
        "    # display optimal parameters.\n",
        "    print(\"The optimal parameters are : \" + str(grid.best_params_) + \" with a score of : \" + str(grid.best_score_) +\n",
        "          \" with merged dataset\")\n",
        "\n",
        "    model = svm.SVC(gamma=grid.best_params_['gamma'], C=grid.best_params_['C'])\n",
        "    model.fit(X_test, y_test)\n",
        "\n",
        "    xx, yy = make_meshgrid(X_train[:, 0], X_train[:, 1], 0.01)\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y, edgecolors='k')\n",
        "\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(\"RBF Kernel with optimal values of C and gamma\")\n",
        "    #plt.savefig('/Users/victorseguin/Desktop/python_plots/plotrbfkerneloptimal.eps', format='eps')\n",
        "    print(\"===========END RBF KERNEL ==========\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}